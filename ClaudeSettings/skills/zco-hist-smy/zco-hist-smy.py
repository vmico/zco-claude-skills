#!/usr/bin/env python3
"""
##;zco-hist-smy: å¯¹è¯å†å²æ±‡æ€»å·¥å…·
##;ç”¨æ³•: zco-hist-smy [-d days]
##;  -d 1   å½“å¤© (é»˜è®¤)
##;  -d 7   è¿‘ 7 å¤©
##;  -d 0   æ‰€æœ‰å†å²
"""

import argparse
import json
import os
import re
import subprocess
from collections import Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple


def get_hist_dir(project_dir: Path = None) -> Path:
    """è·å–å†å²è®°å½•ç›®å½•"""
    hist_dir_name = os.environ.get('ZCO_CHAT_SAVE_DIR', None)
    git_root = get_git_root(project_dir)
    if not hist_dir_name:
        hist_dir = git_root / '_.zco_hist'
    else:
        hist_dir = os.path.abspath(os.path.join(str(git_root), hist_dir_name))
    hist_dir.mkdir(exist_ok=True)
    return hist_dir


def get_git_root(project_dir: Path = None) -> Path:
    """è·å–å½“å‰ Git ä»“åº“æ ¹ç›®å½•"""
    try:
        # æ‰§è¡Œ git rev-parse --show-toplevel å‘½ä»¤
        if project_dir:
            result = subprocess.run(
                ['git', '-C', str(project_dir), 'rev-parse', '--show-toplevel'],
                capture_output=True, text=True, check=True
            )
        else:
            result = subprocess.run(
                ['git', 'rev-parse', '--show-toplevel'],
                capture_output=True, text=True, check=True
            )
        return Path(result.stdout.strip())
    except (subprocess.CalledProcessError, FileNotFoundError):
        return Path.cwd()


def parse_args():
    """##;è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ±‡æ€» _.zco_hist ç›®å½•ä¸‹çš„å¯¹è¯å†å²è®°å½•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  zco-hist-smy        # æ±‡æ€»å½“å¤©
  zco-hist-smy -d 1   # æ±‡æ€»å½“å¤©ï¼ˆæ˜¾å¼ï¼‰
  zco-hist-smy -d 7   # æ±‡æ€»è¿‘ 7 å¤©
  zco-hist-smy -d 0   # æ±‡æ€»æ‰€æœ‰å†å²è®°å½•
        """,
    )
    parser.add_argument(
        "-d",
        "--days",
        type=int,
        default=1,
        help="å¤©æ•°èŒƒå›´ (é»˜è®¤: 1, 0 è¡¨ç¤ºä¸é™)",
    )
    return parser.parse_args()


def calculate_date_range(days: int) -> Tuple[Optional[datetime], datetime]:
    """##;è®¡ç®—æ—¥æœŸèŒƒå›´
    ##;Args:
    ##;    days: å¤©æ•°ï¼Œ0 è¡¨ç¤ºä¸é™
    ##;Returns:
    ##;    (start_date, end_date)ï¼Œstart_date å¯èƒ½ä¸º None
    """
    end_date = datetime.now()
    if days == 0:
        return None, end_date
    start_date = end_date - timedelta(days=days - 1)
    ##;é‡ç½®ä¸ºå½“å¤©å¼€å§‹
    start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
    return start_date, end_date


def get_hist_files(
    hist_dir: Path, start_date: Optional[datetime], end_date: datetime
) -> List[Path]:
    """##;è·å–ç¬¦åˆæ¡ä»¶çš„å¯¹è¯æ–‡ä»¶"""
    if not hist_dir.exists():
        return []

    files = []
    for f in hist_dir.glob("*.md"):
        ##;è·³è¿‡ debug æ–‡ä»¶å’Œæ±‡æ€»æ–‡ä»¶
        if "debug" in f.name or "smy" in f.name:
            continue

        ##;è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        mtime = datetime.fromtimestamp(f.stat().st_mtime)

        ##;æ£€æŸ¥æ˜¯å¦åœ¨æ—¥æœŸèŒƒå›´å†…
        if start_date is None:
            files.append(f)
        elif start_date <= mtime <= end_date:
            files.append(f)

    ##;æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    files.sort(key=lambda x: x.stat().st_mtime)
    return files


def extract_tools_from_content(content: str) -> List[str]:
    """##;ä»å†…å®¹ä¸­æå–å·¥å…·åç§°"""
    tools = []

    ##;åŒ¹é…æŠ˜å é¢æ¿ä¸­çš„å·¥å…·å <summary>ğŸ“„ <b>Read</b>
    pattern1 = r"<summary>.*?<b>(\w+)</b>"
    tools.extend(re.findall(pattern1, content))

    ##;åŒ¹é…å·¥å…·ä½¿ç”¨ç»Ÿè®¡è¡Œ
    pattern2 = r"-\s+(\w+):\s*\d+"
    tools.extend(re.findall(pattern2, content))

    return tools


def extract_files_from_content(content: str) -> List[str]:
    """##;ä»å†…å®¹ä¸­æå–æ–‡ä»¶è·¯å¾„"""
    files = []

    ##;åŒ¹é… ğŸ“„ æ–‡ä»¶è·¯å¾„
    pattern = r"ğŸ“„\s+`?([^`\n]+)`?"
    matches = re.findall(pattern, content)
    files.extend(matches)

    ##;åŒ¹é…ä»£ç å—ä¸­çš„ file_path
    pattern2 = r'"file_path":\s*"([^"]+)"'
    matches = re.findall(pattern2, content)
    files.extend(matches)

    return list(set(files))


def extract_urls_from_content(content: str) -> List[str]:
    """##;ä»å†…å®¹ä¸­æå– URLs"""
    urls = []

    ##;åŒ¹é… ğŸŒ URL
    pattern = r"ğŸŒ\s+(https?://[^\s\n]+)"
    matches = re.findall(pattern, content)
    urls.extend(matches)

    return list(set(urls))


def parse_chat_file(file_path: Path) -> Dict:
    """##;è§£æå•ä¸ªå¯¹è¯æ–‡ä»¶"""
    try:
        content = file_path.read_text(encoding="utf-8")
    except Exception as e:
        return {
            "filename": file_path.name,
            "error": str(e),
            "mtime": datetime.fromtimestamp(file_path.stat().st_mtime),
        }

    ##;æå–åŸºæœ¬ä¿¡æ¯
    mtime = datetime.fromtimestamp(file_path.stat().st_mtime)

    ##;å°è¯•æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€ä¸ª # æ ‡é¢˜ï¼‰
    title_match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
    title = title_match.group(1) if title_match else file_path.stem

    ##;æå–æ—¶é—´ï¼ˆä»æ–‡ä»¶å†…å®¹æˆ–æ–‡ä»¶åï¼‰
    time_match = re.search(r"\*\*æ—¶é—´\*\*[:ï¼š]\s*(.+)", content)
    if time_match:
        try:
            chat_time = datetime.strptime(time_match.group(1).strip(), "%Y-%m-%d %H:%M:%S")
        except ValueError:
            chat_time = mtime
    else:
        chat_time = mtime

    ##;æå–å·¥å…·
    tools = extract_tools_from_content(content)
    tool_counts = Counter(tools)

    ##;æå–æ–‡ä»¶
    files = extract_files_from_content(content)

    ##;æå– URLs
    urls = extract_urls_from_content(content)

    return {
        "filename": file_path.name,
        "title": title,
        "mtime": mtime,
        "chat_time": chat_time,
        "tools": tool_counts,
        "files": files,
        "urls": urls,
        "content_preview": content[:500] if content else "",
    }


def generate_summary(
    files: List[Path], start_date: Optional[datetime], end_date: datetime
) -> Tuple[str, Dict]:
    """##;ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    ##;Returns:
    ##;    (markdown_content, stats_dict)
    """
    ##;è§£ææ‰€æœ‰æ–‡ä»¶
    parsed_files = [parse_chat_file(f) for f in files]
    parsed_files = [p for p in parsed_files if "error" not in p]

    if not parsed_files:
        return "# å¯¹è¯å†å²æ±‡æ€»æŠ¥å‘Š\n\næ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å¯¹è¯è®°å½•ã€‚\n", {}

    ##;ç»Ÿè®¡æ•°æ®
    total_chats = len(parsed_files)
    all_tools = Counter()
    all_files = set()
    all_urls = set()

    for p in parsed_files:
        all_tools.update(p["tools"])
        all_files.update(p["files"])
        all_urls.update(p["urls"])

    total_tools = sum(all_tools.values())

    ##;ç”Ÿæˆ Markdown
    lines = []
    lines.append("# å¯¹è¯å†å²æ±‡æ€»æŠ¥å‘Š")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    ##;æ—¥æœŸèŒƒå›´
    if start_date:
        lines.append(
            f"**ç»Ÿè®¡å‘¨æœŸ**: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
        )
    else:
        lines.append("**ç»Ÿè®¡å‘¨æœŸ**: å…¨éƒ¨å†å²")

    lines.append(f"**æ€»å¯¹è¯æ•°**: {total_chats}")
    lines.append("")
    lines.append("---")
    lines.append("")

    ##;ç»Ÿè®¡æ¦‚è§ˆ
    lines.append("## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ")
    lines.append("")
    lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
    lines.append("|------|------|")
    lines.append(f"| æ€»å¯¹è¯æ•° | {total_chats} |")
    lines.append(f"| ä½¿ç”¨å·¥å…·æ¬¡æ•° | {total_tools} |")
    lines.append(f"| æ¶‰åŠæ–‡ä»¶æ•° | {len(all_files)} |")
    lines.append(f"| è®¿é—® URLs | {len(all_urls)} |")
    lines.append("")

    ##;å·¥å…·ä½¿ç”¨åˆ†å¸ƒ
    if all_tools:
        lines.append("### å·¥å…·ä½¿ç”¨åˆ†å¸ƒ")
        lines.append("")
        lines.append("| å·¥å…· | æ¬¡æ•° | å æ¯” |")
        lines.append("|------|------|------|")

        for tool, count in all_tools.most_common():
            percentage = (count / total_tools * 100) if total_tools > 0 else 0
            lines.append(f"| {tool} | {count} | {percentage:.1f}% |")

        lines.append("")

    ##;å¯¹è¯åˆ—è¡¨
    lines.append("---")
    lines.append("")
    lines.append("## ğŸ“ å¯¹è¯åˆ—è¡¨")
    lines.append("")

    for idx, p in enumerate(parsed_files, 1):
        lines.append(f"### {idx}. {p['filename']}")
        lines.append("")
        lines.append(f"- **æ ‡é¢˜**: {p['title']}")
        lines.append(f"- **æ—¶é—´**: {p['chat_time'].strftime('%Y-%m-%d %H:%M:%S')}")

        if p["tools"]:
            tool_str = ", ".join([f"{t}Ã—{c}" for t, c in p["tools"].most_common()])
            lines.append(f"- **å·¥å…·**: {tool_str}")

        if p["files"]:
            lines.append(f"- **æ–‡ä»¶**: {', '.join(p['files'][:3])}")
            if len(p["files"]) > 3:
                lines.append(f"  - ... ç­‰ {len(p['files'])} ä¸ªæ–‡ä»¶")

        lines.append("")

    ##;æ¶‰åŠæ–‡ä»¶æ±‡æ€»
    if all_files:
        lines.append("---")
        lines.append("")
        lines.append("## ğŸ“ æ¶‰åŠæ–‡ä»¶æ±‡æ€»")
        lines.append("")

        for f in sorted(all_files)[:50]:  # ;æœ€å¤šæ˜¾ç¤º 50 ä¸ª
            lines.append(f"- `{f}`")

        if len(all_files) > 50:
            lines.append(f"- ... ç­‰å…± {len(all_files)} ä¸ªæ–‡ä»¶")

        lines.append("")

    ##;å‚è€ƒèµ„æºæ±‡æ€»
    if all_urls:
        lines.append("---")
        lines.append("")
        lines.append("## ğŸ”— å‚è€ƒèµ„æºæ±‡æ€»")
        lines.append("")

        for url in sorted(all_urls):
            lines.append(f"- [{url}]({url})")

        lines.append("")

    ##;é¡µè„š
    lines.append("---")
    lines.append("")
    lines.append(f"*ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
    lines.append("")

    ##;ç»Ÿè®¡å­—å…¸
    stats = {
        "total_chats": total_chats,
        "total_tools": total_tools,
        "tool_distribution": dict(all_tools),
        "files_count": len(all_files),
        "urls_count": len(all_urls),
    }

    return "\n".join(lines), stats


def main():
    args = parse_args()

    ##;è®¡ç®—æ—¥æœŸèŒƒå›´
    start_date, end_date = calculate_date_range(args.days)

    ##;è·å–é¡¹ç›®æ ¹ç›®å½•
    git_root = get_git_root()

    ##;æŸ¥æ‰¾ _.zco_hist ç›®å½•
    ## hist_dir = git_root / "_.zco_hist"
    hist_dir = get_hist_dir(git_root)
    if not hist_dir.exists():
        print(f"##;@ERROR: æœªæ‰¾åˆ°å¯¹è¯ç›®å½•: {hist_dir}")
        print("è¯·å…ˆå¯ç”¨å¯¹è¯ä¿å­˜åŠŸèƒ½å¹¶æ‰§è¡Œä¸€äº›å¯¹è¯ã€‚")
        return 1

    ##;è·å–æ–‡ä»¶åˆ—è¡¨
    files = get_hist_files(hist_dir, start_date, end_date)

    if not files:
        date_range = (
            f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
            if start_date
            else "å…¨éƒ¨å†å²"
        )
        print(f"##;@NOTE: åœ¨ {date_range} èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°å¯¹è¯è®°å½•")
        return 0

    print(f"##;æ‰¾åˆ° {len(files)} ä¸ªå¯¹è¯æ–‡ä»¶")

    ##;ç”Ÿæˆæ±‡æ€»
    markdown_content, stats = generate_summary(files, start_date, end_date)

    ##;ç¡®å®šè¾“å‡ºç›®å½•
    output_dir = Path(os.environ.get("AICO_DOCS", git_root / "AICO_DOCS"))
    output_dir.mkdir(parents=True, exist_ok=True)

    ##;ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d")
    output_file = output_dir / f"zco_hist_smy_{timestamp}.md"

    ##;å†™å…¥æ–‡ä»¶
    try:
        output_file.write_text(markdown_content, encoding="utf-8")
        print(f"##;æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        print(f"##;ç»Ÿè®¡: {stats.get('total_chats', 0)} ä¸ªå¯¹è¯, {stats.get('total_tools', 0)} æ¬¡å·¥å…·è°ƒç”¨")
    except Exception as e:
        print(f"##;@ERROR: ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
